{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r11cNiLqvWC6"
      },
      "source": [
        "# MicroWakeWord V2 Model Trainer for Google Colab\n",
        "\n",
        "This notebook is specifically for use in Google Colab. The code below will train a basic MicroWakeWord model. It is intended as a **starting point** for advanced users. This notebook needs to be used with Python 3.10, if you elect to use a different version, code changes will be required.\n",
        "\n",
        "This notebook will produce a very rough V2 .tflite model compatible with ESPHome, with no code changes, though you'll need to experiment with the settings to produce a reliable model. This is especially true if you are training a very short or long wake word (especially short or long wake words may be unuseable with default settings). There are comments inline poniting out the most impactful settings and what they affect.\n",
        "\n",
        "> **Configuring the notebook**: You must supply a phoentic and directory-friendly name in step 2 below, and change the runtime type to GPU: T4 for free accounts, or optionally A100 for paid.\n",
        "\tIf you have a _huggingface.co_ account, you are encouraged to provide your API token in a secret named \"HF_TOKEN\", however this is not required.\n",
        "\n",
        "> **Running the notebook**:\n",
        "\tOnce you've supplied your wake word values and any settings you'd like to change, **click the run/play button** on the left of the first card and wait for it to finish. If you see an error about PIP dependency resolver, it is safe to ignore. Once step one has completed, click **Runtime**>**Restart session**. Don't run step 1 again after restarting. After restarting (make sure not to re-run step 1), run each card in order starting at step 2.\n",
        "\n",
        "> **Using the Micro Wake Word V2 .tflite model**:\n",
        "\tUpon completion of this notebook, you will be prompted to download a tflite file. To use this in ESPHome, you need to write a V2 model manifest JSON file. See the [ESPHome documentation](https://esphome.io/components/micro_wake_word) for esphome configuration details and the [model repo](https://github.com/esphome/micro-wake-word-models/tree/main/models/v2) for V2 .json examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFf6511E65ff",
        "outputId": "52bd5432-d871-437c-f35c-661eb6a0f288"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Install MicroWakeWord \n",
        "################################################################################\n",
        "import os\n",
        "\n",
        "###############\n",
        "# You may see an error in the output about PIPs dependency resolver not taking\n",
        "# installed packages into account: this is safe to ignore.\n",
        "###############\n",
        "\n",
        "!git clone https://github.com/kahrendt/microWakeWord.git\n",
        "!pip install -e ./microWakeWord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "dEluu7nL7ywd",
        "outputId": "92f5816e-f743-4db5-9d97-5b30120beba9"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Define wake word, set up some output directories, and generate a sample of \n",
        "# the wake word to play for verifation\n",
        "#\n",
        "# Use a 'phonetic' spelling of your wake word when setting target_word below\n",
        "# Once this step completes, you'll here a sample of what the expected pronunciation will be.\n",
        "# You are free to adjust target_word and re-run this step until the wake word sounds \n",
        "# like what you expect to speak\n",
        "################################################################################\n",
        "\n",
        "target_word = \"hey wahbee\"  # Phonetic spellings produce better samples for many words (try running this sample, and then change back to 'hey wabi' and run again for an example)\n",
        "target_word_friendly = \"hey_wabi\" # Directory safe non-phonetic spelling of wake word (use _ in place of spaces)\n",
        "\n",
        "###############\n",
        "# If you have a Hugging Face API key, make sure it's in a Colab secret named \"HF_SECRET\"\n",
        "###############\n",
        "\n",
        "hf_api_token_secret_name = \"HF_SECRET\"\n",
        "\n",
        "\n",
        "###############\n",
        "# Should not need to change these\n",
        "###############\n",
        "\n",
        "archive_download_path = \"./tmp\"\n",
        "\n",
        "rir_wav_path = \"./mit_rirs_16k\"\n",
        "\n",
        "audioset_wav_path = \"./audioset_16k\"\n",
        "# AudioSet is an enormous dataset, limit to 35% by default, which is enough to \n",
        "# produce a great model and will save time. Change to 100 for maximum precision \n",
        "audioset_sample_limit_pct = 35\n",
        "\n",
        "fma_wav_path = \"./fma_16k\"\n",
        "\n",
        "expected_dirs = [ archive_download_path, rir_wav_path, audioset_wav_path, fma_wav_path ]\n",
        "\n",
        "for dir in expected_dirs:\n",
        "  if not os.path.exists(dir):\n",
        "    os.mkdir(dir)\n",
        "\n",
        "output_ext = \".wav\"\n",
        "\n",
        "generated_samples_output_dir = f\"generated_samples/{target_word_friendly}\"\n",
        "\n",
        "print(f\"Using {generated_samples_output_dir} for Piper generated wake word samples\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import Audio\n",
        "\n",
        "if not os.path.exists(\"./piper-sample-generator\"):\n",
        "  !git clone https://github.com/rhasspy/piper-sample-generator\n",
        "\n",
        "if not os.path.isfile(\"./piper-sample-generator/models/en_US-libritts_r-medium.pt\"):\n",
        "  !wget -q -c --show-progress -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
        "\n",
        "  # Install system dependencies\n",
        "  !pip install torch\n",
        "  !pip install torchaudio\n",
        "  !pip install piper-phonemize\n",
        "\n",
        "if \"piper-sample-generator/\" not in sys.path:\n",
        "  sys.path.append(\"piper-sample-generator/\")\n",
        "\n",
        "!python3 piper-sample-generator/generate_samples.py \"{target_word}\" \\\n",
        "--max-samples 1 \\\n",
        "--batch-size 1 \\\n",
        "--output-dir \"{generated_samples_output_dir}\"\n",
        "\n",
        "Audio(f\"{generated_samples_output_dir}/0.wav\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SvGtCCM9akR",
        "outputId": "780bb720-de49-4494-d999-e54b64dd450d"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Generates a large number of wake word samples\n",
        "#\n",
        "# Start here when trying to improve your model.\n",
        "# See https://github.com/rhasspy/piper-sample-generator for the full set of\n",
        "# parameters. In particular, experiment with noise-scales and noise-scale-ws,\n",
        "# generating negative samples similar to the wake word, and generating many more\n",
        "# wake word samples, possibly with different phonetic pronunciations.\n",
        "################################################################################\n",
        "\n",
        "!python3 piper-sample-generator/generate_samples.py \"{target_word}\" \\\n",
        "--model piper-sample-generator/models/en_US-libritts_r-medium.pt \\\n",
        "--max-samples 2048 \\\n",
        "--batch-size 92 \\\n",
        "--max-speakers 777 \\\n",
        "--noise-scales 0.667 \\\n",
        "--noise-scale-ws 0.667 \\\n",
        "--length-scales 0.85 \\\n",
        "--length-scales 1.0 \\\n",
        "--length-scales 1.15 \\\n",
        "--output-dir \"{generated_samples_output_dir}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541,
          "referenced_widgets": [
            "3d5c855c43e0472ba229eab23203c7cd",
            "2106de7880ca4abd967388f9c5f42fb5",
            "5f14a0d7e12e41e889640b0133a94cae",
            "12abe23b215a423f81c062e414228db7",
            "7dd3d8efac6440c0b0590c7a8e47c7d9",
            "3ec0baf53efe4a85a34ddc2980334916",
            "81ac302fb38a4576bfaa9cee6672daf7",
            "ff05e4f1275b4687b401819237d890a1",
            "2bda21b917fc44e99ed1ce79a05440e6",
            "fbecaad16eab4da9b0cc6338d0e5cb73",
            "570d64760a3b4a1f8cd8f7fe63777200",
            "64f0cc5f39084a558d83b0e7924f91e9",
            "9035aa6ca5134b52952e3fdd53fd2555",
            "49ef7f2c8067411db959f73bfd7ed921",
            "d5d17547831940c3acc1d44ee7660e12",
            "081172aaab3d47bb9e05143e02ad8319",
            "756bfc1bd9fe4876a69babfa602dd9a7",
            "ae0d95101c1f497c9fd4cd04a510017e",
            "b3a8b63b877c4a7aa1b5fd118fd896ed",
            "32f14a3000fe42be8547ac26f482159a",
            "0a868fa950c04686ba9a9666df4d27df",
            "d11c7a4733244ee385c2cb30061f489e"
          ]
        },
        "collapsed": true,
        "id": "YJRG4Qvo9nXG",
        "outputId": "c5d3a32a-92b5-4f2f-bf0e-fd4dbd1aa0ab"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Download audio data for augmentation\n",
        "#\n",
        "# Based on openWakeWord's automatic_model_training.ipynb; March 4, 2024\n",
        "#\n",
        "# IMPORTANT! The data downloaded here has a mixture of different\n",
        "# licenses and usage restrictions. As such, any custom models trained with this\n",
        "# data should be considered as appropriate for personal use only. You must seek\n",
        "# a license with rights' owners for any commercial use.\n",
        "#\n",
        "# This can take 30-45 minutes\n",
        "################################################################################\n",
        "\n",
        "import datasets\n",
        "import scipy\n",
        "import os\n",
        "import soundfile\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from pathlib import Path\n",
        "from datasets import IterableDataset, Dataset, DownloadConfig, load_dataset, DatasetInfo\n",
        "from typing import Dict, Any\n",
        "from google.colab import userdata\n",
        "\n",
        "from __future__ import generators\n",
        "\n",
        "@dataclass\n",
        "class ExistingFileMapping:\n",
        "    FilesPresent: Dict[str, Path]\n",
        "    InitialPath: str \n",
        "\n",
        "class FileMapper:\n",
        "    file_mapping: ExistingFileMapping\n",
        "    inital_path: str\n",
        "\n",
        "    def __init__(self, path: str, file_mapping: ExistingFileMapping = ExistingFileMapping({},\"\")):\n",
        "        self.file_mapping = ExistingFileMapping(FilesPresent={}, InitialPath=path)\n",
        "        \n",
        "        if (file_mapping is not None):\n",
        "          self.file_mapping = file_mapping\n",
        "        \n",
        "        if (not os.path.isdir(path)):\n",
        "           os.mkdir(path)\n",
        "        \n",
        "        self.inital_path = path\n",
        "\n",
        "    def __getattr__(self, item):\n",
        "        return getattr(self.file_provider, item)\n",
        "    \n",
        "    def find_files_under_path_by_ext(self, file_ext: str):\n",
        "       if (self.file_mapping is None or not len(self.file_mapping.InitialPath) > 1 or not len(file_ext) > 1):\n",
        "        self.file_mapping.FilesPresent = {}\n",
        "        return\n",
        "       \n",
        "       self.file_mapping.FilesPresent = {x.name:x for x in Path(f\"{self.file_mapping.InitialPath}/\").glob(f\"**/*.{file_ext}\")}\n",
        "\n",
        "    def contains_filename(self, filename:str) -> bool:\n",
        "        return (self.file_mapping.FilesPresent.get(filename) is not None)\n",
        "\n",
        "\n",
        "################################################\n",
        "# Local WAV file producer for datasets\n",
        "# Avoids an \"issue\" in FLAC that can cause libsndfile errors with try except recursion\n",
        "#\n",
        "# Should the script fail or session get interrupted during the download and conversion process (which can \n",
        "# be long running), we will not reprocess anything that already exists\n",
        "#\n",
        "# Note: this function preserves dataset async streaming, use streaming=true where possible when loading datasets\n",
        "# Warning: if you call Dataset.to_iterable_dataset, to_list, etc, the entire dataset will be downloaded and hydrated syncronously\n",
        "################################################\n",
        "def create_wav_files_from_dataset(dataset:Dataset, output_path: str):\n",
        "  i = 0\n",
        "\n",
        "  # Initialize small class to handle tracking files of particular type that already exist under output_path\n",
        "  existing_file_mapper:FileMapper = FileMapper(output_path)\n",
        "\n",
        "  # create file_mapper dict with existing .wav files under output directory\n",
        "  existing_file_mapper.find_files_under_path_by_ext(output_ext)\n",
        "\n",
        "  dataset_info:DatasetInfo = dataset.info\n",
        "\n",
        "  dataset_info_output_path = f\"{output_path}/dataset_info\"\n",
        "\n",
        "  if not os.path.exists(dataset_info_output_path):\n",
        "    os.mkdir(dataset_info_output_path)\n",
        "\n",
        "  dataset_info.write_to_directory(f\"{output_path}/dataset_info\")\n",
        "  dataset_size_bytes:int = dataset_info.size_in_bytes or 0\n",
        "  dataset_num_rows:int = 0\n",
        "  \n",
        "  if hasattr(dataset_info, 'num_samples'):\n",
        "    dataset_num_rows = dataset_info.num_samples\n",
        "  elif type(dataset) is Dataset:\n",
        "    dataset_num_rows = dataset.num_rows\n",
        "\n",
        "  # at the cost of readibility, use builtin filter to reduce dataset to only files that still need to be converted and saved\n",
        "  # function(example: Union[Dict, Any]) -> bool if with_indices=False, batched=False\n",
        "  if \"path\" in dataset:\n",
        "    unprocessed_audio = dataset.filter(lambda ds_row: not existing_file_mapper.contains_filename(os.path.basename(ds_row[\"file\"])), \\\n",
        "                                        input_columns=[ \"path\" ], \\\n",
        "                                        with_indices=False)\n",
        "  else:\n",
        "    unprocessed_audio = dataset \n",
        "  \n",
        "  # casting column is the last thing to happen before writing out .wavs to disk\n",
        "  # shuffle here to avoid shuffling in a recursive loop\n",
        "  unprocessed_audio = unprocessed_audio \\\n",
        "                        .cast_column(\"audio\", datasets.Audio(sampling_rate=16000)) \\\n",
        "                        .shuffle(seed=42)\n",
        "  \n",
        "  ds_size_kb = round(dataset_size_bytes / 1024)\n",
        "  ds_size_disp = f\"{round(ds_size_kb / 1024, ndigits=2)}MB\" if ds_size_kb > 1024 else f\"{ds_size_kb}KB\"\n",
        "\n",
        "  \n",
        "  print(\"Processing Dataset\")\n",
        "  print(f\"\\tDataset size: {ds_size_disp}\")\n",
        "  \n",
        "  if dataset_num_rows > 0:\n",
        "    print(f\"\\tConverting {dataset_num_rows} clips to 16-bit WAV...\\n\")\n",
        "\n",
        "\n",
        "  try:\n",
        "    for row in unprocessed_audio:\n",
        "        i += 1\n",
        "        \n",
        "        # if dataset doesn't have path column we're never saving time just create a file every time\n",
        "        dest_wav_file_name = f\"{i}{output_ext}\"\n",
        "        \n",
        "        # but if it does, write out a wav with the original filename\n",
        "        if \"path\" in row:\n",
        "          dest_wav_file_name = os.path.splitext( \\\n",
        "                                                  os.path.basename(row[\"path\"]))[0] \\\n",
        "                                                  + f\"{output_ext}\"\n",
        "        \n",
        "        dest_wav_file_path = os.path.join(output_path, dest_wav_file_name)\n",
        "\n",
        "        scipy \\\n",
        "            .io \\\n",
        "            .wavfile \\\n",
        "            .write(filename=dest_wav_file_path, \\\n",
        "                  rate=16000, \\\n",
        "                  data=(row[\"audio\"][\"array\"]*32767).astype(np.int16))\n",
        "  except soundfile.LibsndfileError:\n",
        "      print(\"Failed to convert row audio data to WAV, skipping file\\n\")\n",
        "      # Here's where I learned a long lesson on indentation\n",
        "      create_wav_files_from_dataset(dataset.skip(i + 1), output_path)\n",
        "  return\n",
        "\n",
        "\n",
        "################################################\n",
        "# Reference only: manually create dataset from local files\n",
        "# Instead use load_dataset, which has a handler for local files\n",
        "################################################\n",
        "def create_dataset_from_files_under_path(path: str, fileext: str):\n",
        "  file_dict:Dict[str, Path] = { x.name:x for x in Path(f\"{path}/\").glob(f\"**/*.{fileext}\")}\n",
        "  files_dataset = datasets.Dataset.from_dict({ \"audio\": [Path(p) for p in file_dict.values]})    \n",
        "  \n",
        "  return files_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "\n",
        "################################################\n",
        "# Simple dataset dowwload config provider\n",
        "################################################\n",
        "\n",
        "def get_download_config(dataset_name: str) -> DownloadConfig:\n",
        "  use_hf_secret_for_token = True\n",
        "  hf_api_token: str = \"\"\n",
        "\n",
        "  try:\n",
        "    hf_api_token = userdata.get(f\"{hf_api_token_secret_name}\")\n",
        "  except userdata.SecretNotFoundError:\n",
        "    use_hf_secret_for_token = False\n",
        "  dataset_download_config = datasets.DownloadConfig(\n",
        "      cache_dir=f\"{dataset_name}/cache\",\n",
        "      token=hf_api_token if use_hf_secret_for_token else False,\n",
        "      )\n",
        "\n",
        "  return dataset_download_config\n",
        "\n",
        "################################################\n",
        "# Load a dataset\n",
        "# If you know there is no path data, set save_infos=true\n",
        "# and we'll try to salvage what we can from DatasetInfo\n",
        "#\n",
        "# For now, default torch_format to False as we're using\n",
        "# datasets.IterableDataset, rather than a torch model\n",
        "################################################\n",
        "def retrieve_dataset(path:str, \\\n",
        "                     download_config:DownloadConfig, \\\n",
        "                     split:str = \"train\", \\\n",
        "                     sample_limit_pct:int = 100, \\\n",
        "                     streaming:bool = True, \\\n",
        "                     save_infos:bool = False, \\\n",
        "                     torch_format:bool = False):\n",
        "  \n",
        "  if (sample_limit_pct < 100):\n",
        "     split = f\"train[:{sample_limit_pct}%]\"\n",
        "\n",
        "  loaded_ds = datasets.load_dataset(path, \\\n",
        "                                  split=split, \\\n",
        "                                  streaming=streaming, \\\n",
        "                                  download_config=download_config, \\\n",
        "                                  save_infos=save_infos,\n",
        "                               )\n",
        "\n",
        "  return loaded_ds.with_format(\"torch\") if torch_format else loaded_ds\n",
        "\n",
        "################################################\n",
        "# Download MIT RIR data\n",
        "################################################\n",
        "\n",
        "print(\"Retrieving MIT RIR dataset\\n\")\n",
        "\n",
        "download_config=get_download_config(\"mit_rir\")\n",
        "rir_dataset = retrieve_dataset(path = \"davidscripka/MIT_environmental_impulse_responses\", \\\n",
        "                               download_config = download_config)\n",
        "\n",
        "create_wav_files_from_dataset(rir_dataset, rir_wav_path)\n",
        "\n",
        "\n",
        "################################################\n",
        "# Audioset annoated audio training data\n",
        "# (https://research.google.com/audioset/dataset/index.html)\n",
        "# Download the audioset .tar files, extract, and convert to 16khz\n",
        "# For full-scale training, it's recommended to download the entire dataset from\n",
        "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
        "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
        "################################################\n",
        "\n",
        "print(\"Retrieving Audioset dataset\\n\")\n",
        "\n",
        "download_config=get_download_config(\"audioset\")\n",
        "audioset_dataset = retrieve_dataset(path = \"agkphysics/Audioset\", \\\n",
        "                               download_config = download_config, \\\n",
        "                               sample_limit_pct = audioset_sample_limit_pct)\n",
        "\n",
        "create_wav_files_from_dataset(audioset_dataset, audioset_wav_path)\n",
        "\n",
        "\n",
        "################################################\n",
        "# Free Music Archive dataset\n",
        "# https://github.com/mdeff/fma\n",
        "# (Third-party mchl914 extra small set)\n",
        "################################################\n",
        "\n",
        "print(\"Retrieving FMA dataset\\n\")\n",
        "\n",
        "fma_archive_filename = \"fma_xs.zip\"\n",
        "fma_archive_url = f\"https://huggingface.co/datasets/mchl914/fma_xsmall/resolve/main/{fma_archive_filename}\"\n",
        "\n",
        "fma_dataset = load_dataset(\"audiofolder\", \\\n",
        "                          data_files=f\"{fma_archive_url}\", \\\n",
        "                          DownloadConfig=get_download_config(\"fma\"))\n",
        "\n",
        "create_wav_files_from_dataset(fma_dataset, fma_wav_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW3bmbI5-JAz"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Set up the augmentations\n",
        "# To improve your model, experiment with these settings and use more sources of\n",
        "# background clips.\n",
        "################################################################################\n",
        "\n",
        "from microwakeword.audio.augmentation import Augmentation\n",
        "from microwakeword.audio.clips import Clips\n",
        "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
        "\n",
        "clips = Clips(input_directory=f\"{generated_samples_output_dir}\",\n",
        "              file_pattern='*.wav',\n",
        "              max_clip_duration_s=None,\n",
        "              remove_silence=False,\n",
        "              random_split_seed=10,\n",
        "              split_count=0.1,\n",
        "              )\n",
        "augmenter = Augmentation(augmentation_duration_s=3.2,\n",
        "                         augmentation_probabilities = {\n",
        "                                \"SevenBandParametricEQ\": 0.1,\n",
        "                                \"TanhDistortion\": 0.1,\n",
        "                                \"PitchShift\": 0.1,\n",
        "                                \"BandStopFilter\": 0.1,\n",
        "                                \"AddColorNoise\": 0.1,\n",
        "                                \"AddBackgroundNoise\": 0.75,\n",
        "                                \"Gain\": 1.0,\n",
        "                                \"RIR\": 0.5,\n",
        "                            },\n",
        "                         impulse_paths = ['mit_rirs'],\n",
        "                         background_paths = ['fma_16k', 'audioset_16k'],\n",
        "                         background_min_snr_db = -5,\n",
        "                         background_max_snr_db = 10,\n",
        "                         min_jitter_s = 0.195,\n",
        "                         max_jitter_s = 0.205,\n",
        "                         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "V5UsJfKKD1k9",
        "outputId": "ccadffbc-a218-450a-e99e-d151787d5164"
      },
      "outputs": [],
      "source": [
        "# Augment a random clip and play it back to verify it works well\n",
        "\n",
        "from IPython.display import Audio\n",
        "from microwakeword.audio.audio_utils import save_clip\n",
        "\n",
        "random_clip = clips.get_random_clip()\n",
        "augmented_clip = augmenter.augment_clip(random_clip)\n",
        "save_clip(augmented_clip, \"augmented_clip.wav\")\n",
        "\n",
        "Audio(\"augmented_clip.wav\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7BHcY1mEGbK"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Augment samples and save the training, validation, and testing sets.\n",
        "# Validating and testing samples generated the same way can make the model\n",
        "# benchmark better than it performs in real-word use. Use real samples or TTS\n",
        "# samples generated with a different TTS engine to potentially get more accurate\n",
        "# benchmarks.\n",
        "#\n",
        "# lelando - to-do: since mww seems to support streaming, and training data is \n",
        "# streaming, next step should be eliminating .wav output except as necessary\n",
        "# to e.g Audio[autoplay=true]\n",
        "################################################################################\n",
        "\n",
        "import os\n",
        "from mmap_ninja.ragged import RaggedMmap\n",
        "\n",
        "if not os.path.exists(\"generated_augmented_features\"):\n",
        "  os.mkdir(\"generated_augmented_features\")\n",
        "\n",
        "generated_features_output_dir = f\"generated_augmented_features/{target_word_friendly}\"\n",
        "\n",
        "if not os.path.exists(generated_features_output_dir):\n",
        "    os.mkdir(generated_features_output_dir)\n",
        "\n",
        "splits = [\"training\", \"validation\", \"testing\"]\n",
        "for split in splits:\n",
        "  out_dir = os.path.join(generated_features_output_dir, split)\n",
        "  if not os.path.exists(out_dir):\n",
        "      os.mkdir(out_dir)\n",
        "\n",
        "  split_name = \"train\"\n",
        "  repetition = 2\n",
        "\n",
        "  spectrograms = SpectrogramGeneration(clips=clips, \\\n",
        "                                     augmenter=augmenter, \\\n",
        "                                     slide_frames=10, \\   # Uses the same spectrogram repeatedly, just shifted over by one frame. This simulates the streaming inferences while training/validating in nonstreaming mode.\n",
        "                                     step_ms=10)\n",
        "  if split == \"validation\":\n",
        "    split_name = \"validation\"\n",
        "    repetition = 1\n",
        "  elif split == \"testing\":\n",
        "    split_name = \"test\"\n",
        "    repetition = 1\n",
        "    spectrograms = SpectrogramGeneration(clips=clips,\n",
        "                                     augmenter=augmenter,\n",
        "                                     slide_frames=1,    # The testing set uses the streaming version of the model, so no artificial repetition is necessary\n",
        "                                     step_ms=10,\n",
        "                                     )\n",
        "\n",
        "  RaggedMmap.from_generator(\n",
        "      out_dir=os.path.join(out_dir, f'{target_word_friendly}_mmap'),\n",
        "      sample_generator=spectrograms.spectrogram_generator(split=split_name, repeat=repetition),\n",
        "      batch_size=92,\n",
        "      verbose=True,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pGuJDPyp3ax"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Download pre-generated spectrogram features (made for microWakeWord in\n",
        "# particular) for various negative datasets.\n",
        "# This can be slow!\n",
        "#\n",
        "# lelando - to-do: use retrieve_dataset\n",
        "################################################################################\n",
        "\n",
        "output_dir = './negative_datasets'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
        "    filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']\n",
        "    for fname in filenames:\n",
        "        link = link_root + fname\n",
        "\n",
        "        zip_path = f\"negative_datasets/{fname}\"\n",
        "        !wget -O {zip_path} {link}\n",
        "        !unzip -q {zip_path} -d {output_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii1A14GsGVQT"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Save the training configuration YAML file\n",
        "# IMPORTANT! These hyperparamters can make a huge different in model quality.\n",
        "# Experiment with sampling and penalty weights and increasing the number of\n",
        "# training steps.\n",
        "################################################################################\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "config = {}\n",
        "\n",
        "config[\"window_step_ms\"] = 10\n",
        "\n",
        "config[\"train_dir\"] = (\n",
        "    f\"trained_models/{target_word_friendly}\"\n",
        ")\n",
        "\n",
        "\n",
        "# Each feature_dir should have at least one of the following folders with this structure:\n",
        "#  training/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#\n",
        "#  sampling_weight: Weight for choosing a spectrogram from this set in the batch\n",
        "#  penalty_weight: Penalizing weight for incorrect predictions from this set\n",
        "#  truth: Boolean whether this set has positive samples or negative samples\n",
        "#  truncation_strategy = If spectrograms in the set are longer than necessary for training, how are they truncated\n",
        "#       - random: choose a random portion of the entire spectrogram - useful for long negative samples\n",
        "#       - truncate_start: remove the start of the spectrogram\n",
        "#       - truncate_end: remove the end of the spectrogram\n",
        "#       - split: Split the longer spectrogram into separate spectrograms offset by 100 ms. Only for ambient sets\n",
        "\n",
        "config[\"features\"] = [\n",
        "    {\n",
        "        \"features_dir\": f'{generated_features_output_dir}',\n",
        "        \"sampling_weight\": 2.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": True,\n",
        "        \"truncation_strategy\": \"truncate_start\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"negative_datasets/speech\",\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"negative_datasets/dinner_party\",\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"negative_datasets/no_speech\",\n",
        "        \"sampling_weight\": 5.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    { # Only used for validation and testing\n",
        "        \"features_dir\": \"negative_datasets/dinner_party_eval\",\n",
        "        \"sampling_weight\": 0.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"split\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Number of training steps in each iteration - various other settings are configured as lists that corresponds to different steps\n",
        "config[\"training_steps\"] = [10000]\n",
        "\n",
        "# Penalizing weight for incorrect class predictions - lists that correspond to training steps\n",
        "config[\"positive_class_weight\"] = [1]\n",
        "config[\"negative_class_weight\"] = [20]\n",
        "\n",
        "config[\"learning_rates\"] = [\n",
        "    0.001,\n",
        "]  # Learning rates for Adam optimizer - list that corresponds to training steps\n",
        "config[\"batch_size\"] = 92\n",
        "\n",
        "config[\"time_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"time_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "\n",
        "config[\"eval_step_interval\"] = (\n",
        "    500  # Test the validation sets after every this many steps\n",
        ")\n",
        "config[\"clip_duration_ms\"] = (\n",
        "    1300  # Maximum length of wake word that the streaming model will accept\n",
        ")\n",
        "\n",
        "# The best model weights are chosen first by minimizing the specified minimization metric below the specified target_minimization\n",
        "# Once the target has been met, it chooses the maximum of the maximization metric. Set 'minimization_metric' to None to only maximize\n",
        "# Available metrics:\n",
        "#   - \"loss\" - cross entropy error on validation set\n",
        "#   - \"accuracy\" - accuracy of validation set\n",
        "#   - \"recall\" - recall of validation set\n",
        "#   - \"precision\" - precision of validation set\n",
        "#   - \"false_positive_rate\" - false positive rate of validation set\n",
        "#   - \"false_negative_rate\" - false negative rate of validation set\n",
        "#   - \"ambient_false_positives\" - count of false positives from the split validation_ambient set\n",
        "#   - \"ambient_false_positives_per_hour\" - estimated number of false positives per hour on the split validation_ambient set\n",
        "config[\"target_minimization\"] = 0.9\n",
        "config[\"minimization_metric\"] = None  # Set to None to disable\n",
        "\n",
        "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
        "\n",
        "with open(os.path.join(\"training_parameters.yaml\"), \"w\") as file:\n",
        "    documents = yaml.dump(config, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoEXJBaiC9mf"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Train the model\n",
        "# When finished, the code will quantize and convert the model to a\n",
        "# streaming version suitable for on-device detection.\n",
        "# This will resume if stopped, but it will start over at the configured training\n",
        "# steps in the yaml file.\n",
        "# Change --train 0 to only convert and test the best-weighted model.\n",
        "# Google Colab does not print mini-batch results, so it may appear\n",
        "# stuck for several minutes.\n",
        "# This can be slow!\n",
        "################################################################################\n",
        "\n",
        "!python -m microwakeword.model_train_eval \\\n",
        "--training_config='training_parameters.yaml' \\\n",
        "--train 1 \\\n",
        "--restore_checkpoint 1 \\\n",
        "--test_tf_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming_quantized 0 \\\n",
        "--test_tflite_streaming 0 \\\n",
        "--test_tflite_streaming_quantized 1 \\\n",
        "--use_weights \"best_weights\" \\\n",
        "mixednet \\\n",
        "--pointwise_filters \"64,64,64,64\" \\\n",
        "--repeat_in_block  \"1, 1, 1, 1\" \\\n",
        "--mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
        "--residual_connection \"0,0,0,0\" \\\n",
        "--first_conv_filters 32 \\\n",
        "--first_conv_kernel_size 5 \\\n",
        "--stride 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex_UIWvwtjAN"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Download the tflite model file\n",
        "# To use on the wake word with ESPHome, you need to write a\n",
        "# V2 model JSON file. See https://esphome.io/components/micro_wake_word for the\n",
        "# documentation and\n",
        "# https://github.com/esphome/micro-wake-word-models/tree/main/models/v2 for\n",
        "# examples of the V2 json format. Adjust the probability threshold based on\n",
        "# the test results obtained.\n",
        "# After the training is complete, you may also need to increase the Tensor arena\n",
        "# model size if the model later fails to load in ESPHome.\n",
        "################################################################################\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Prompt user to download their model .tflite. (to-do?) Might be nice to implemnt something to generate\n",
        "# a basic v2 .json since we have all the relavent values, then add both files to an archive and \n",
        "# offer that for download instead \n",
        "tflite_output_dir = f\"trained_models/{target_word_friendly}/tflite_stream_state_internal_quant\"\n",
        "model_tflite_filename =f'trained_models/{target_word_friendly}/{target_word_friendly}.tflite'\n",
        "os.rename(f'{tflite_output_dir}/stream_state_internal_quant.tflite', model_tflite_filename)\n",
        "\n",
        "files.download(f\"{model_tflite_filename}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "081172aaab3d47bb9e05143e02ad8319": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a868fa950c04686ba9a9666df4d27df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12abe23b215a423f81c062e414228db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbecaad16eab4da9b0cc6338d0e5cb73",
            "placeholder": "​",
            "style": "IPY_MODEL_570d64760a3b4a1f8cd8f7fe63777200",
            "value": " 270/270 [00:00&lt;00:00,  2.29it/s]"
          }
        },
        "2106de7880ca4abd967388f9c5f42fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ec0baf53efe4a85a34ddc2980334916",
            "placeholder": "​",
            "style": "IPY_MODEL_81ac302fb38a4576bfaa9cee6672daf7",
            "value": "Resolving data files: 100%"
          }
        },
        "2bda21b917fc44e99ed1ce79a05440e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32f14a3000fe42be8547ac26f482159a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d5c855c43e0472ba229eab23203c7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2106de7880ca4abd967388f9c5f42fb5",
              "IPY_MODEL_5f14a0d7e12e41e889640b0133a94cae",
              "IPY_MODEL_12abe23b215a423f81c062e414228db7"
            ],
            "layout": "IPY_MODEL_7dd3d8efac6440c0b0590c7a8e47c7d9"
          }
        },
        "3ec0baf53efe4a85a34ddc2980334916": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49ef7f2c8067411db959f73bfd7ed921": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3a8b63b877c4a7aa1b5fd118fd896ed",
            "max": 882,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32f14a3000fe42be8547ac26f482159a",
            "value": 882
          }
        },
        "570d64760a3b4a1f8cd8f7fe63777200": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f14a0d7e12e41e889640b0133a94cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff05e4f1275b4687b401819237d890a1",
            "max": 270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bda21b917fc44e99ed1ce79a05440e6",
            "value": 270
          }
        },
        "64f0cc5f39084a558d83b0e7924f91e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9035aa6ca5134b52952e3fdd53fd2555",
              "IPY_MODEL_49ef7f2c8067411db959f73bfd7ed921",
              "IPY_MODEL_d5d17547831940c3acc1d44ee7660e12"
            ],
            "layout": "IPY_MODEL_081172aaab3d47bb9e05143e02ad8319"
          }
        },
        "756bfc1bd9fe4876a69babfa602dd9a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dd3d8efac6440c0b0590c7a8e47c7d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ac302fb38a4576bfaa9cee6672daf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9035aa6ca5134b52952e3fdd53fd2555": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_756bfc1bd9fe4876a69babfa602dd9a7",
            "placeholder": "​",
            "style": "IPY_MODEL_ae0d95101c1f497c9fd4cd04a510017e",
            "value": "Resolving data files: 100%"
          }
        },
        "ae0d95101c1f497c9fd4cd04a510017e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3a8b63b877c4a7aa1b5fd118fd896ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d11c7a4733244ee385c2cb30061f489e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5d17547831940c3acc1d44ee7660e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a868fa950c04686ba9a9666df4d27df",
            "placeholder": "​",
            "style": "IPY_MODEL_d11c7a4733244ee385c2cb30061f489e",
            "value": " 882/882 [00:00&lt;00:00,  3.59it/s]"
          }
        },
        "fbecaad16eab4da9b0cc6338d0e5cb73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff05e4f1275b4687b401819237d890a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
